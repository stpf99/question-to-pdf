"""
Pseudo System 2 AI - PDF Query System (wersja z spaCy)
Wymaga: pip install pypdf2 spacy
"""

import re
import argparse
import sys
from typing import List, Dict, Tuple
import spacy

# Za≈Çaduj model spaCy
try:
    nlp = spacy.load("en_core_web_sm")
    print("‚úì Za≈Çadowano model spaCy")
except OSError:
    print("Pobieranie modelu spaCy...")
    from spacy.cli import download
    download("en_core_web_sm")
    nlp = spacy.load("en_core_web_sm")
    print("‚úì Za≈Çadowano model spaCy")

class PseudoSystem2AI:
    def __init__(self):
        self.document_words = []  # [(word, pos_tag, sentence_id, word_id)]
        self.sentences = []  # Lista zda≈Ñ
        self.stop_words = nlp.Defaults.stop_words

    def load_pdf(self, pdf_path: str):
        """Wczytaj PDF i przetw√≥rz na s≈Çowa z tagami"""
        try:
            import PyPDF2

            # WyciƒÖgnij tekst z PDF
            with open(pdf_path, 'rb') as file:
                reader = PyPDF2.PdfReader(file)
                text = ""
                for page in reader.pages:
                    text += page.extract_text()

            self._process_text(text)
            print(f"‚úì Wczytano {len(self.sentences)} zda≈Ñ, {len(self.document_words)} s≈Ç√≥w")

        except Exception as e:
            print(f"B≈ÇƒÖd wczytywania PDF: {e}")
            raise  # Przeka≈º b≈ÇƒÖd dalej

    def _process_text(self, text: str):
        """Przetw√≥rz tekst na s≈Çowa z tagami gramatycznymi"""
        # Podziel na zdania
        doc = nlp(text)
        self.sentences = [sent.text for sent in doc.sents]

        # Przetw√≥rz ka≈ºde zdanie
        for sent_id, sentence in enumerate(self.sentences):
            sent_doc = nlp(sentence)

            # Zapisz s≈Çowa z tagami
            for word_id, token in enumerate(sent_doc):
                self.document_words.append({
                    'word': token.text.lower(),
                    'original': token.text,
                    'pos': token.pos_,
                    'sentence_id': sent_id,
                    'word_id': word_id
                })

    def _analyze_question(self, question: str) -> Dict:
        """
        Przeanalizuj pytanie - wyciƒÖgnij czasownik i rzeczownik
        """
        doc = nlp(question)

        verbs = []
        nouns = []
        adjectives = []

        for token in doc:
            word_lower = token.text.lower()

            # Pomi≈Ñ stop words
            if word_lower in self.stop_words:
                continue

            if token.pos_ == "VERB":  # Czasownik
                verbs.append(word_lower)
            elif token.pos_ == "NOUN":  # Rzeczownik
                nouns.append(word_lower)
            elif token.pos_ == "ADJ":  # Przymiotnik
                adjectives.append(word_lower)

        return {
            'verbs': verbs,
            'nouns': nouns,
            'adjectives': adjectives,
            'all_keywords': verbs + nouns + adjectives
        }

    def _find_matching_sentences(self, keywords: List[str], min_matches: int = 1) -> List[int]:
        """Znajd≈∫ zdania zawierajƒÖce s≈Çowa kluczowe"""
        sentence_matches = {}  # sentence_id: count

        for entry in self.document_words:
            if entry['word'] in keywords:
                sent_id = entry['sentence_id']
                sentence_matches[sent_id] = sentence_matches.get(sent_id, 0) + 1

        # Sortuj wed≈Çug liczby dopasowa≈Ñ (malejƒÖco)
        sorted_sentences = sorted(
            sentence_matches.items(),
            key=lambda x: x[1],
            reverse=True
        )

        # Zwr√≥ƒá tylko te z minimum dopasowa≈Ñ
        return [s_id for s_id, count in sorted_sentences if count >= min_matches]

    def query(self, question: str, max_results: int = 3) -> List[str]:
        """
        Zadaj pytanie do systemu

        Args:
            question: Pytanie po angielsku
            max_results: Maksymalna liczba wynik√≥w

        Returns:
            Lista znalezionych zda≈Ñ
        """
        if not self.document_words:
            return ["‚ö† Najpierw wczytaj dokument u≈ºywajƒÖc load_pdf() lub load_text()"]

        # Analiza pytania
        analysis = self._analyze_question(question)

        print(f"\nüîç Analiza pytania:")
        print(f"   Czasowniki: {analysis['verbs']}")
        print(f"   Rzeczowniki: {analysis['nouns']}")
        print(f"   Przymiotniki: {analysis['adjectives']}")

        if not analysis['all_keywords']:
            return ["‚ö† Nie znaleziono s≈Ç√≥w kluczowych w pytaniu"]

        # Znajd≈∫ pasujƒÖce zdania
        matching_sent_ids = self._find_matching_sentences(
            analysis['all_keywords'],
            min_matches=1
        )

        if not matching_sent_ids:
            return ["‚ö† Nie znaleziono pasujƒÖcych fragment√≥w w dokumencie"]

        # Zwr√≥ƒá zdania
        results = []
        for sent_id in matching_sent_ids[:max_results]:
            results.append(self.sentences[sent_id])

        return results

def main():
    # Konfiguracja argument√≥w wiersza polece≈Ñ
    parser = argparse.ArgumentParser(description='System do odpowiadania na pytania z PDF')
    parser.add_argument('-q', '--question', required=True, help='Pytanie do zadanego u≈ºytkownika')
    parser.add_argument('-f', '--file', required=True, help='≈öcie≈ºka do pliku PDF')
    parser.add_argument('-r', '--results', type=int, default=3, help='Maksymalna liczba wynik√≥w (domy≈õlnie: 3)')

    args = parser.parse_args()

    # Inicjalizacja systemu
    ai = PseudoSystem2AI()

    try:
        # Wczytaj PDF
        ai.load_pdf(args.file)

        # Zadaj pytanie
        print(f"\n‚ùì Pytanie: {args.question}")
        print("-" * 60)

        results = ai.query(args.question, max_results=args.results)

        # Wy≈õwietl wyniki
        if results:
            for i, result in enumerate(results, 1):
                print(f"\n{i}. {result}")
        else:
            print("\n‚ö† Nie znaleziono pasujƒÖcych odpowiedzi w dokumencie")

    except Exception as e:
        print(f"\n‚ùå B≈ÇƒÖd: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
